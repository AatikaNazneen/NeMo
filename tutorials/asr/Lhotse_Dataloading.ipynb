{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14eb898-0f8e-4783-a70e-ab54b4de910d",
   "metadata": {},
   "source": [
    "# Tutorial: NeMo & Lhotse Data Loading\n",
    "\n",
    "![image](https://raw.githubusercontent.com/lhotse-speech/lhotse/master/docs/logo.png)\n",
    "\n",
    "In this tutorial we introduce the integration of NeMo with Lhotse, a library for speech data preparation and loading. Lhotse adds new capabilities to NeMo, allowing to move certain operations such as bucketing or dataset blending to dataloading runtime, rather than dataset preparation. This allows a greater flexibility in trying out various dataset setups without the need to prepare different data variants ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eada3f-d3df-4038-b927-c666ec2b8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import nemo\n",
    "import lhotse\n",
    "from lhotse import CutSet\n",
    "from lhotse.recipes import (\n",
    "    download_librispeech, \n",
    "    download_yesno, \n",
    "    prepare_librispeech, \n",
    "    prepare_yesno,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2b7ad-be34-4518-aa74-44e8253c0a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_root = str(Path(nemo.__path__[0]).parent)\n",
    "root_dir = Path(\"data\")\n",
    "root_dir.mkdir(parents=True, exist_ok=True)\n",
    "num_jobs = os.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6709798-78ec-477e-86da-8110f307f6f5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Quick Lhotse Primer\n",
    "\n",
    "[Lhotse](https://github.com/lhotse-speech/lhotse) is a toolkit for speech data handling that originated as a part of the next-generation Kaldi framework. Next-gen Kaldi includes other tools, such as [k2](https://github.com/k2-fsa/k2), which is also [integrated into NeMo](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/experimental/k2/speech_to_text_bpe.py).\n",
    "\n",
    "Lhotse represents speech data as python objects, and provides efficient sampling and dataloading mechanisms. The core Lhotse concepts leveraged in NeMo integration are:\n",
    "- Representation for individual examples: [Recording](https://lhotse.readthedocs.io/en/latest/corpus.html#recording-manifest), [SupervisionSegment](https://lhotse.readthedocs.io/en/latest/corpus.html#supervision-manifest), and [Cut](https://lhotse.readthedocs.io/en/latest/cuts.html#cuts).\n",
    "- Representation for a dataset and/or mini-batch: [CutSet](https://lhotse.readthedocs.io/en/latest/api.html#lhotse.cut.CutSet).\n",
    "- Samplers (stratified and non-stratified): [DynamicCutSampler](https://lhotse.readthedocs.io/en/latest/datasets.html#lhotse.dataset.sampling.DynamicCutSampler) and [DynamicBucketingSampler](https://lhotse.readthedocs.io/en/latest/datasets.html#lhotse.dataset.sampling.DynamicBucketingSampler).\n",
    "- A specific method of blending multiple datasets together, based on a probabilistic multiplexer: [CutSet.mux](https://lhotse.readthedocs.io/en/latest/api.html?highlight=mux#lhotse.cut.CutSet.mux).\n",
    "\n",
    "### How is Lhotse integrated into NeMo?\n",
    "\n",
    "Like NeMo, Lhotse leverages JSON Lines (JSONL) format to keep metadata in manifests. Unlike NeMo, Lhotse's manifests are mapped into Python objects. Most data represented by NeMo manifests are a special case supported by Lhotse and can be adapted on-the-fly to be directly read as a Lhotse `CutSet`. This conversion is enabled by [LazyNeMoIterator](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/common/data/lhotse/nemo_adapters.py#L33) and [LazyTarredNeMoIterator](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/common/data/lhotse/nemo_adapters.py#L131) classes inside NeMo.\n",
    "\n",
    "Building a `DataLoader` with Lhotse relies heavily on `CutSampler` to sample a mini-batch on metadata level as a `CutSet`, which is then passed to a map-style PyTorch dataset that converts the `CutSet` to a tuple/dict of tensors. This is covered in more detail in Lhotse tutorials:\n",
    "- Introductory tutorial [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/00-basic-workflow.ipynb)\n",
    "- Tutorial on Lhotse Shar format (\"tarred\" data format in Lhotse): [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/04-lhotse-shar.ipynb)\n",
    "\n",
    "The dataloader is being constructed in NeMo from a dataset configuration using function [get_lhotse_dataloader_from_config](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/common/data/lhotse/dataloader.py#L103).\n",
    "\n",
    "### Getting mini LibriSpeech\n",
    "\n",
    "For the sake of the tutorial, we'll be using very small datasets to keep things quick. The first example is mini LibriSpeech, a 5h train set and 2h dev set collection that was released alongside the popular LibriSpeech dataset. Lhotse provides a download function that fetches and unpacks the archive with data, and a prepare function that creates Lhotse manifests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4033f-ec05-4ead-9675-72dc3d36a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_root = download_librispeech(root_dir, dataset_parts=\"mini_librispeech\")\n",
    "\n",
    "libri = prepare_librispeech(\n",
    "    libri_root, output_dir=root_dir, num_jobs=num_jobs\n",
    ")\n",
    "\n",
    "for split in (\"train-clean-5\", \"dev-clean-2\"):\n",
    "    (\n",
    "        CutSet\n",
    "        .from_manifests(**libri[split])\n",
    "        .to_file(f\"data/librispeech_cuts_{split}.jsonl.gz\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33922226-01ba-4e21-b148-bf520e7946af",
   "metadata": {},
   "source": [
    "# Training NeMo models with Lhotse \n",
    "\n",
    "For quick illustration, we'll fine-tune a small NeMo model `nvidia/stt_en_conformer_ctc_small` for 100 steps, running a validation every 50 steps. When training from scratch, you'd simply use a different NeMo script, while keeping the same dataloading options (perhaps tuning the batch size settings to fit your model choice).\n",
    "\n",
    "**Technical note.** We train using existing NeMo `speech_to_text_finetune.py` script to illustrate real-world usage patterns of NeMo. If you're unfamiliar with running bash commands from a Jupyter notebook, starting the line with an exclamation mark `!` runs a single command, and starting a cell with `%%bash -s {var}` runs the whole cell in bash, and passes `var` as argument `$1` (subsequent -s args will be passed as `$2`, `$3`, and so on). \n",
    "\n",
    "**Common arguments.** Let's briefly discuss the CLI arguments we provide to the fine-tuning script.\n",
    "- Most of the options are in `model.train_ds` and `model.validation_ds` namespaces:\n",
    "  - `use_lhotse=true` enables Lhotse dataloading backend\n",
    "  - Batch size is dynamic and controlled via `batch_duration`, `use_bucketing`, `num_buckets`, and `quadratic_duration`. [Please refer to **NeMo documentation** for details](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/datasets.html#enabling-lhotse-via-configuration).\n",
    "  - We set `batch_size=null` explicitly (the base config has some value that would have otherwise limited our dynamic batch size).\n",
    "  - Although we don't enable dynamic batch size for validation in this example for brevity, it does also work (including with bucketing).\n",
    "- `trainer.use_distributed_sampler=false` is required by Lhotse (it has its own distributed sampling handling).\n",
    "- In `trainer` namespace, `max_steps` controls the total number of steps in training; `val_check_interval` is the number of steps between validation runs.\n",
    "- The `+` notation is used to append a value to a config (i.e., requires when these are not present in the YAML config file). [See **Hydra override syntax** for details](https://hydra.cc/docs/1.1/advanced/override_grammar/basic/#basic-override-syntax).\n",
    "\n",
    "**Model inference.** We skip inference in this tutorial for brevity. [Please refer to **NeMo ASR inference documentation** to learn more](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/asr/intro.html).\n",
    "\n",
    "## I. Training using Lhotse CutSet\n",
    "\n",
    "Let's start with training from our existing Lhotse manifests for mini LibriSpeech. Highlights for relevant CLI arguments:\n",
    "- The manifest path is provided via `cuts_path` so the trainer knows to read this as a Lhotse manifest.\n",
    "- We have to set `manifest_filepath=null` explicitly as older NeMo configs expect some value to be provided there regardless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdcb5d-047b-44ec-94c9-20e83c37142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/examples/asr/speech_to_text_finetune.py \\\n",
    "    +init_from_pretrained_model=\"nvidia/stt_en_conformer_ctc_small\" \\\n",
    "    name=\"finetune_from_cutset\" \\\n",
    "    +model.train_ds.use_lhotse=true \\\n",
    "    model.train_ds.manifest_filepath=null \\\n",
    "    +model.train_ds.cuts_path=data/librispeech_cuts_train-clean-5.jsonl.gz \\\n",
    "    model.train_ds.batch_size=null \\\n",
    "    +model.train_ds.batch_duration=300 \\\n",
    "    +model.train_ds.use_bucketing=true \\\n",
    "    +model.train_ds.num_buckets=30 \\\n",
    "    +model.train_ds.quadratic_duration=15 \\\n",
    "    +model.validation_ds.use_lhotse=true \\\n",
    "    model.validation_ds.manifest_filepath=null \\\n",
    "    +model.validation_ds.cuts_path=data/librispeech_cuts_dev-clean-2.jsonl.gz \\\n",
    "    model.validation_ds.batch_size=64 \\\n",
    "    +trainer.use_distributed_sampler=false \\\n",
    "    trainer.max_steps=100 \\\n",
    "    ++trainer.val_check_interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772c3e1-163d-4b34-a1c0-9efd117b7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nemo_experiments/finetune_from_cutset/*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4860e2-631a-4522-bf0f-4471cfc46545",
   "metadata": {},
   "source": [
    "## II. Training using NeMo manifest\n",
    "\n",
    "Training from NeMo manifest format is done in a similar way. Highlights:\n",
    "- We'll convert Lhotse CutSet manifests to NeMo manifests for this exercise. If you have existing data in NeMo format, just use it as-is.\n",
    "- We can now use `manifest_filepath` argument directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929a59c-1375-4397-b2fb-07ccc03f6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in (\"train-clean-5\", \"dev-clean-2\"):\n",
    "    lhotse.serialization.save_to_jsonl(\n",
    "        (\n",
    "            {\n",
    "                \"audio_filepath\": cut.recording.sources[0].source,\n",
    "                \"duration\": cut.duration,\n",
    "                \"text\": cut.supervisions[0].text,\n",
    "                \"lang\": cut.supervisions[0].language,\n",
    "                \"sampling_rate\": cut.sampling_rate,\n",
    "            }\n",
    "            for cut in CutSet.from_file(f\"data/librispeech_cuts_{split}.jsonl.gz\")\n",
    "        ),\n",
    "        f\"data/nemo_{split}.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822dc33c-98a2-4c39-bc04-b274a7422e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/examples/asr/speech_to_text_finetune.py \\\n",
    "    +init_from_pretrained_model=\"nvidia/stt_en_conformer_ctc_small\" \\\n",
    "    name=\"finetune_from_nemo_json\" \\\n",
    "    +model.train_ds.use_lhotse=true \\\n",
    "    model.train_ds.manifest_filepath=data/nemo_train-clean-5.json \\\n",
    "    model.train_ds.batch_size=null \\\n",
    "    +model.train_ds.batch_duration=300 \\\n",
    "    +model.train_ds.use_bucketing=true \\\n",
    "    +model.train_ds.num_buckets=30 \\\n",
    "    +model.train_ds.quadratic_duration=15 \\\n",
    "    +model.validation_ds.use_lhotse=true \\\n",
    "    model.validation_ds.manifest_filepath=data/nemo_dev-clean-2.json \\\n",
    "    model.validation_ds.batch_size=64 \\\n",
    "    +trainer.use_distributed_sampler=false \\\n",
    "    trainer.max_steps=100 \\\n",
    "    ++trainer.val_check_interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdff9bc-078f-4b88-85ec-18a9f7c86017",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nemo_experiments/finetune_from_nemo_json/*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6cc48-159c-44e3-a666-6b42f5b1587d",
   "metadata": {},
   "source": [
    "## III. Training using NeMo tarred manifest\n",
    "\n",
    "Tarred manifest format is useful when dealing with I/O bottlenecks. Typical scenarios are compute grids with shared resources, magnetic disks, cloud storage such as AWS S3, GCS, Azure Blob Storage, etc. \n",
    "\n",
    "We'll cover NeMo native tarred format below. If you're interested in Lhotse Shar tarred format, these also work in NeMo -- [please visit the **relevant Lhotse Shar tutorial** for details](https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/04-lhotse-shar.ipynb). \n",
    "\n",
    "Highlights:\n",
    "- First, let's convert data to NeMo tarred manifests using dedicated NeMo script `convert_to_tarred_audio_dataset.py`. We'll split the data into 32 shards here.\n",
    "- For training, we'll use `manifest_filepath` for sharded JSONL manifests, and `tarred_audio_filepaths` for sharded audio tar files. Note the NeMo-specific syntax of `_OP_0..31_CL_` which tells NeMo to expand it into a list of 32 items.\n",
    "- The only other modification is that we're adding `trainer.train_limit_batches` option which tells the trainer what's the size of a pseudo-epoch. In previous examples we could have measured training length in epochs, but chose not to. However, with tarred datasets, we completely discard the notion of an epoch -- the data iterator is infinite, so we use the count of steps for everything instead. This design prevents issues with hanging due to uneven dataloader lengths in distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89de72-7edc-4356-9863-f9e6e1b9eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/scripts/speech_recognition/convert_to_tarred_audio_dataset.py \\\n",
    "    --manifest_path data/nemo_train-clean-5.json \\\n",
    "    --num_shards 32 \\\n",
    "    --workers 4 \\\n",
    "    --max_duration 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf18615-df6a-45a3-9e7f-aba7749bc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/examples/asr/speech_to_text_finetune.py \\\n",
    "    +init_from_pretrained_model=\"nvidia/stt_en_conformer_ctc_small\" \\\n",
    "    name=\"finetune_from_nemo_tarred\" \\\n",
    "    +model.train_ds.use_lhotse=true \\\n",
    "    model.train_ds.manifest_filepath=tarred/sharded_manifests/manifest__OP_0..31_CL_.json \\\n",
    "    model.train_ds.tarred_audio_filepaths=tarred/audio__OP_0..31_CL_.tar \\\n",
    "    model.train_ds.batch_size=null \\\n",
    "    +model.train_ds.batch_duration=300 \\\n",
    "    +model.train_ds.use_bucketing=true \\\n",
    "    +model.train_ds.num_buckets=30 \\\n",
    "    +model.train_ds.quadratic_duration=15 \\\n",
    "    +model.validation_ds.use_lhotse=true \\\n",
    "    model.validation_ds.manifest_filepath=data/nemo_dev-clean-2.json \\\n",
    "    model.validation_ds.batch_size=64 \\\n",
    "    +trainer.use_distributed_sampler=false \\\n",
    "    trainer.max_steps=100 \\\n",
    "    +trainer.limit_train_batches=50 \\\n",
    "    ++trainer.val_check_interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abb816-7908-4b33-a0e7-e60965636b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nemo_experiments/finetune_from_nemo_tarred/*/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d97bb-e6ed-4ea9-9df0-5baeb1bd38c6",
   "metadata": {},
   "source": [
    "## IV. Dynamically mixing multiple NeMo tarred datasets\n",
    "\n",
    "Let's introduce another feature of NeMo+Lhotse dataloading: mixing multiple datasets together. \n",
    "\n",
    "Lhotse supports a special type of data mixing that we call **weighted stochastic multiplexing**. It means that when we iterate multiple independent data sources, at each step we'll sample one source to pick the next item from according to the source weights. When the weight is not provided, we count the number of elements in each source before starting dataloading and use those as \"natural\" weights (this can be time-expensive, so it's best to precompute and provide those explicitly). This sampling strategy ensures that each mini-batch consists of a roughly constant blend of data from multiple sources throughout the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce9b4c-df69-4849-af16-5aefbb800997",
   "metadata": {},
   "source": [
    "### Fetch another dataset (yesno)\n",
    "\n",
    "For these experiments, we download another small dataset called `yesno`. It's a bunch of recordings where a single person says either \"yes\" or \"no\" in Hebrew, with transcriptions in English (so it's technically a speech translation task). We convert the data to NeMo tarred format, same as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51688267-c5c6-4a8d-bd99-7684320d11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno_root = download_yesno(root_dir)\n",
    "\n",
    "yesno = prepare_yesno(yesno_root, output_dir=root_dir)\n",
    "\n",
    "for split in (\"train\", \"test\"):\n",
    "    cuts = CutSet.from_manifests(**yesno[split])\n",
    "    cuts.to_file(f\"data/yesno_cuts_{split}.jsonl.gz\")\n",
    "    lhotse.serialization.save_to_jsonl(\n",
    "        (\n",
    "            {\n",
    "                \"audio_filepath\": cut.recording.sources[0].source,\n",
    "                \"duration\": cut.duration,\n",
    "                \"text\": cut.supervisions[0].text,\n",
    "                \"lang\": cut.supervisions[0].language,\n",
    "                \"sampling_rate\": cut.sampling_rate,\n",
    "            }\n",
    "            for cut in cuts\n",
    "        ),\n",
    "        f\"data/yesno_{split}.json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc78d14-f769-4535-8da2-02b1c1a2466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/scripts/speech_recognition/convert_to_tarred_audio_dataset.py \\\n",
    "    --manifest_path data/yesno_train.json \\\n",
    "    --num_shards 1 \\\n",
    "    --workers 1 \\\n",
    "    --max_duration 30 \\\n",
    "    --target_dir yesno_tarred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dddc26-8f97-4d78-b2ac-9b5218d7da4b",
   "metadata": {},
   "source": [
    "### [optional] Maximum efficiency: precomputing bucket duration bins\n",
    "\n",
    "We'll use a NeMo script `estimate_duration_bins.py` to precompute the optimal bucketing settings for our data blend. The result will be passed to the training script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e5f4c-9378-4009-bdf5-fec8d8b0624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash -s {nemo_root}\n",
    "# python $1/scripts/speech_recognition/estimate_duration_bins.py \\\n",
    "#     --buckets 30 \\\n",
    "#     '[[tarred/sharded_manifests/manifest__OP_0..31_CL_.json,0.8],[yesno_tarred/tarred_audio_manifest.json,0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3909254b-fde8-46bd-9b05-c21da9396a3a",
   "metadata": {},
   "source": [
    "### Training with NeMo tarred manifests from two separate datasets\n",
    "\n",
    "The training command is similar to the examples above. Highlights:\n",
    "- Note that `manifest_filepath` and `tarred_audio_filepaths` now use a special list syntax for providing multiple data sources. Both lists need to have the same order of datasets.\n",
    "- The list in `manifest_filepath` is additionally specifying the weight for each dataset. The weights can be greater than one, and will be automatically re-normalized later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522cc88-187d-43fd-b299-95cbf6dac391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {nemo_root}\n",
    "python $1/examples/asr/speech_to_text_finetune.py \\\n",
    "    +init_from_pretrained_model=\"nvidia/stt_en_conformer_ctc_small\" \\\n",
    "    name=\"finetune_from_nemo_multidataset\" \\\n",
    "    +model.train_ds.use_lhotse=true \\\n",
    "    model.train_ds.manifest_filepath=[[tarred/sharded_manifests/manifest__OP_0..31_CL_.json,0.8],[yesno_tarred/tarred_audio_manifest.json,0.2]] \\\n",
    "    model.train_ds.tarred_audio_filepaths=[[tarred/audio__OP_0..31_CL_.tar],[yesno_tarred/audio_0.tar]] \\\n",
    "    model.train_ds.batch_size=null \\\n",
    "    +model.train_ds.batch_duration=300 \\\n",
    "    +model.train_ds.use_bucketing=true \\\n",
    "    +model.train_ds.num_buckets=30 \\\n",
    "    +model.train_ds.quadratic_duration=15 \\\n",
    "    +model.validation_ds.use_lhotse=true \\\n",
    "    model.validation_ds.manifest_filepath=[data/nemo_dev-clean-2.json,data/yesno_test.json] \\\n",
    "    model.validation_ds.batch_size=64 \\\n",
    "    +trainer.use_distributed_sampler=false \\\n",
    "    trainer.max_steps=100 \\\n",
    "    +trainer.limit_train_batches=50 \\\n",
    "    ++trainer.val_check_interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b0d83-69fe-4f52-91e3-ca67a9be61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nemo_experiments/finetune_from_nemo_multidataset/*/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5817f02-8532-4a03-83b7-8c7863e26049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
